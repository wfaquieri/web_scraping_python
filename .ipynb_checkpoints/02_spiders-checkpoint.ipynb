{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfe8cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web crawlers with scrapy\n",
    "\n",
    "# Required imports\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# The actual spyder\n",
    "class SpiderClasName(scrapy.spider):\n",
    "    name = \"spyder_name\"\n",
    "    #the code for your spyder\n",
    "    \n",
    "\n",
    "# Running the spider\n",
    "process = CrawlerProcess()\n",
    "process.craw(SpiderClassName)\n",
    "\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c3d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weaving the web\n",
    "\n",
    "class DCspider(scrapy.spider):\n",
    "    \n",
    "    name = \"dc_spider\"\n",
    "    \n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.datacamp.com/courses/all']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url = url, callback = self.parse)\n",
    "            \n",
    "    def parse(self, response)\n",
    "    # simple example: write out the html\n",
    "    html_file = 'DC_courses.html'\n",
    "    write open(html_file, wb) as fout:\n",
    "        fout.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3209bfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inheriting the Spider\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    pass\n",
    "  # parse method\n",
    "  def parse(self, response):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class(YourSpider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a974e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hurl the URLs\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = [\"https://www.datacamp.com\", \"https://scrapy.org\"]\n",
    "    for url in urls:\n",
    "      yield url\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e0748c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Self Referencing is Classy\n",
    "\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    self.print_msg( \"Hello World!\" )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  # print_msg method\n",
    "  def print_msg( self, msg ):\n",
    "    print( \"Calling start_requests in YourSpider prints out:\", msg )\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec30d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with Start Requests\n",
    "# Import scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the spider class\n",
    "class YourSpider( scrapy.Spider ):\n",
    "  name = \"your_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = \"https://www.datacamp.com\", callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    pass\n",
    "  \n",
    "# Inspect Your Class\n",
    "inspect_class( YourSpider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1635353f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCamp Course Links: Save to File\n",
    "\n",
    "# Create the spider class\n",
    "class DCSpider( scrapy.Spider ):\n",
    "    \n",
    "  name = \"dcspider\"\n",
    "\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls\n",
    "    yield scrapy.Request( url = url, callback = self.parse )\n",
    "  \n",
    "  # parse method\n",
    "  def parse(self, response)\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    filepath = 'DC_links.csv'\n",
    "    write open(html_file, 'w') as f:\n",
    "        f.writelines([link + '\\n' for link in links])\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc5c922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCamp Course Links: Parse again\n",
    "\n",
    "# Create the spider class\n",
    "class DCSpider( scrapy.Spider ):\n",
    "    \n",
    "  name = \"dcspider\"\n",
    "\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    urls = ['https://www.datacamp.com/courses/all']\n",
    "    for url in urls\n",
    "    yield scrapy.Request( url = url, callback = self.parse )\n",
    "  \n",
    "  # parse method\n",
    "  def parse(self, response)\n",
    "    links = response.css('div.course-block > a::attr(href)').extract()\n",
    "    for link in links:\n",
    "        yield response.follow(url = link, callback = self.parse2)\n",
    " def parse2(self, response)\n",
    "    # parse the course sites here! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d79235",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Obtendo o nome dos autores\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider( scrapy.Spider ):\n",
    "  name = 'dcspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  # parse method\n",
    "  def parse( self, response ):\n",
    "    # Create an extracted list of course author names\n",
    "    author_names = response.css( 'p.course-block__author-name::text' ).extract()\n",
    "    # Here we will just return the list of Authors\n",
    "    return author_names\n",
    "  \n",
    "# Inspect the spider\n",
    "inspect_spider( DCspider )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a566eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawler Time\n",
    "\n",
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr( scrapy.Spider ):\n",
    "  name = 'dcdescr'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request( url = url_short, callback = self.parse )\n",
    "  \n",
    "  # First parse method\n",
    "  def parse( self, response ):\n",
    "    links = response.css( 'div.course-block > a::attr(href)' ).extract()\n",
    "    # Follow each of the extracted links\n",
    "    for link in links:\n",
    "      yield response.follow(url = link, callback = self.parse_descr)\n",
    "      \n",
    "  # Second parsing method\n",
    "  def parse_descr( self, response ):\n",
    "    # Extract course description\n",
    "    course_descr = response.css( 'p.course__description::text' ).extract_first()\n",
    "    # For now, just yield the course description\n",
    "    yield course_descr\n",
    "\n",
    "\n",
    "# Inspect the spider\n",
    "inspect_spider( DCdescr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e3f4ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Capstone # Pedra angular\n",
    "\n",
    "# Parsing the Front Page\n",
    "def parse_front(self, response):\n",
    "    # Narrow in on the course blocks \n",
    "    course_blocks = response.css('div.course-block')\n",
    "    # Direct to the course links\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    # Extract the links (as a list of strings)\n",
    "    links_to_follow = course_links.extract()\n",
    "    # Follow the links to the next parser\n",
    "    for url in links_to_follow:\n",
    "        yield respone.follow(url = url, callback = self.parse_pages)\n",
    "        \n",
    "# Parsing the Course Pages\n",
    "def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"tittle\")]/text()')\n",
    "    # Extract and clean the course title text\n",
    "    crs_title_ext = crs_title.extract_first.strip()\n",
    "    # Direct to the chapter titles text\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    # Extract and clean the chapet title text\n",
    "    ch_titles_ext = [t.strip() for t in crs_title_ext.extract()]\n",
    "    # Store this in our dictionary\n",
    "    dc_dict[ch_titles_ext] = ch_titles_ext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce19df4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    ch_titles = response.css('h4.chapter__title::text')\n",
    "    ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "    dc_dict[ crs_title_ext ] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207664ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataCamp Descriptions\n",
    "\n",
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "url_short = 'https://assets.datacamp.com/production/repositories/2560/datasets/19a0a26daa8d9db1d920b5d5607c19d6d8094b3b/all_short'\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "  name = \"dc_chapter_spider\"\n",
    "  # start_requests method\n",
    "  def start_requests(self):\n",
    "    yield scrapy.Request(url = url_short,\n",
    "                         callback = self.parse_front)\n",
    "  # First parsing method\n",
    "  def parse_front(self, response):\n",
    "    course_blocks = response.css('div.course-block')\n",
    "    course_links = course_blocks.xpath('./a/@href')\n",
    "    links_to_follow = course_links.extract()\n",
    "    for url in links_to_follow:\n",
    "      yield response.follow(url = url,\n",
    "                            callback = self.parse_pages)\n",
    "  # Second parsing method\n",
    "  def parse_pages(self, response):\n",
    "    # Create a SelectorList of the course titles text\n",
    "    crs_title = response.xpath('//h1[contains(@class,\"title\")]/text()')\n",
    "    # Extract the text and strip it clean\n",
    "    crs_title_ext = crs_title.extract_first().strip()\n",
    "    # Create a SelectorList of course descriptions text\n",
    "    crs_descr = response.css( 'p.course__description::text' )\n",
    "    # Extract the text and strip it clean\n",
    "    crs_descr_ext = crs_descr.extract_first().strip()\n",
    "    # Fill in the dictionary\n",
    "    dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34281801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capstone Crawler\n",
    "\n",
    "# parse method\n",
    "def parse(self, response):\n",
    "  # Extracted course titles\n",
    "  crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "  # Extracted course descriptions\n",
    "  crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "  # Fill in the dictionary: it is the spider output\n",
    "  for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "    dc_dict[crs_title] = crs_descr\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f4c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "\n",
    "# Import the CrawlerProcess\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "  name = 'yourspider'\n",
    "  # start_requests method\n",
    "  def start_requests( self ):\n",
    "    yield scrapy.Request(url = url_short, callback = self.parse)\n",
    "      \n",
    "  def parse(self, response):\n",
    "    # My version of the parser you wrote in the previous part\n",
    "    crs_titles = response.xpath('//h4[contains(@class,\"block__title\")]/text()').extract()\n",
    "    crs_descrs = response.xpath('//p[contains(@class,\"block__description\")]/text()').extract()\n",
    "    for crs_title, crs_descr in zip( crs_titles, crs_descrs ):\n",
    "      dc_dict[crs_title] = crs_descr\n",
    "    \n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Run the Spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "process.start()\n",
    "\n",
    "# Print a preview of courses\n",
    "previewCourses(dc_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
